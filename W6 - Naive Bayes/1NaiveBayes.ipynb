{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective**\n",
    "In this week, we will implement Naive Bayes (NB) classifier from scratch.\n",
    "\n",
    "### **Naive Bayes Classifier**\n",
    "\n",
    "* Naive bayes classifier is a **generative classifier**.\n",
    "\n",
    "* It **estimates probability** of a sample belonging to a class using **Bayes theorem** :\n",
    "\\begin{align} \n",
    "\\text {posterior} &=& \\frac{\\text {prior} \\times \\text{likelihood} } { \\text {evidence} }\n",
    "\\\\ p(y|\\mathbf x) &=& \\frac{p(y) \\times p(\\mathbf x |y) }{p(\\mathbf x)} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "![Image source](https://editor.analyticsvidhya.com/uploads/374484.png)\n",
    "\n",
    "* It simplifies the calculation of **likelihood** with **conditional independence assumption** i.e NB assumes that the features are conditionally independent.\n",
    "\n",
    "    * The **likelihood** can be expressed as : \n",
    "    \\begin{align} \n",
    "    p(\\mathbf x| y) &=& p(x_1,x_2,\\ldots,x_m|y) \n",
    "    \\\\&=& p(x_1|y) \\ p(x_2|y) \\ldots\\ p(x_m|y) \\\\\n",
    "    \\end{align}\n",
    "\n",
    "    \\begin{align}\n",
    "    &=& \\prod_{j=1}^{m}p(x_j|y) \n",
    "    \\end{align}\n",
    "\n",
    "    * Substituting likelihood in the Bayes theorem gives us the following formula :\n",
    "    \\begin{equation}\n",
    "    p(y=y_c|\\mathbf x)= \\frac{p(y_c)\\prod_{j=1}^{m}p(x_j|y_c)}{\\sum_{r=1}^kp(y_r) \\prod_{j=1}^{m}p(x_j|y_r)}\n",
    "    \\end{equation}\n",
    "\n",
    "* The algorithm is called **Naive** because of the assumption that 2 variables are independent when they may not be. In a real-world scenario, there is hardly any situation where the features are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inference**\n",
    "* We perform this calculation in **log-space** to avoid issues with underflow due to multiplication of small numbers.\n",
    "\n",
    "* The label that results in the **highest value of the numerator** i.e $ { \\text {likelihood} \\times \\text{prior}}$ is assigned to the given example. \n",
    "\n",
    "**Note** : *The evidence is fixed for all labels and acts as a normalizing constant.* \n",
    "\n",
    "\\begin{equation} \n",
    "y = \\text {argmax}_y \\log p(y) + \\sum_{j=1}^{m}\\log p(x_j|y) \n",
    "\\end{equation}\n",
    "\n",
    "* Posterior probability however needs full calculation of the Bayes formula:\n",
    "\n",
    "    * We first product of likelihood and prior for each label in log space:\n",
    "    \\begin{equation} \n",
    "    \\log p(\\mathbf x, y_r) =\\log p(y_r) + \\sum_{j=1}^{m} \n",
    "    \\log p(x_j|y_r) \n",
    "    \\end{equation} \n",
    "    \n",
    "        and convert that to probability by taking :\n",
    "\n",
    "    \\begin{equation} \n",
    "    p(\\mathbf x, y_r)= {\\exp (\\log p(\\mathbf x,y_r))}\n",
    "    \\end{equation}\n",
    "\n",
    "    * Summing up these probablities for obtaining the evidence or the denominator of the formula.\n",
    "    \\begin{equation} \n",
    "    p(\\mathbf x) = \\sum_{r=1}^{k} p(\\mathbf x,y_r)=\\sum_{r=1}^{k} \\exp (\\log p(\\mathbf x,y_r)) \n",
    "    \\end{equation}\n",
    "\n",
    "    * Substituting these values one can obtain the posterior probability.\n",
    "    \\begin{equation} \n",
    "    p(y_r|\\mathbf x) = \\frac{p(\\mathbf x, y_r)}{p(\\mathbf x)}\n",
    "    \\end{equation}\n",
    "    \n",
    "NB classifier is used in applicaiton like **document classification** and **spam filtering**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Learning problem**\n",
    "\n",
    "* $k$ prior probabilities to be estimated : $\\{p(y_1),p(y_2),\\ldots,p(y_k)\\}$\n",
    "\n",
    "* $k \\times m$ class conditional probabilities : $\\{p(x_1|y_1,\\ldots, p(x_m|y_1,p(x_1|y_2),\\ldots,p(x_m|y_2),\\ldots,p(x_1|y_k),\\ldots,p(x_m|y_k)\\}$\n",
    "\n",
    "The class conditional densities depend on the **nature of features**.\n",
    "\n",
    "The following are some popular class conditional densities used in NB classifier: \n",
    "\n",
    "* **Bernoulli distribution**: When $x_j$ is a **binary feature**, we use Bernoulli distribution to model the class conditional density : $p(x_j|y_c)$.\n",
    "\n",
    "\n",
    "* **Categorical distribution**: When $x_j$ is a **categorical feature** i.e. it takes one of the $e \\gt 2$ discrete values \\[e.g. {red,green,blue} or roll of a dice\\], we use categorical distribution to model the class conditional density : $p(x_j|y_c)$\n",
    "\n",
    "* **Multinomial distribution**: When $\\mathbf x$ is a **count vector** i.e. each component $x_j$ is a count of apperance in the object it represents and $\\sum x_j=l$, which is the length of the object, we use multinomial distribution to model $p(\\mathbf x|y_c)$\n",
    "\n",
    "* **Gaussian distribution**: When $x_j$ is a **continuous feature** i.e. it takes a real value, we use gaussian (or normal) distribution to model the class conditional density $p(x_j|y_c)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will implement these different class conditional densities in different NB implementations. \n",
    "\n",
    "We will discuss parameter estimation in detail in the respective sections."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
