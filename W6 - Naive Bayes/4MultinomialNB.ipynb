{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multinomial Naive Bayes : Implementation**\n",
    "We use **Multinomial NB classifier** for problems like **document classification**.\n",
    "\n",
    "* We represent $i^{th}$ document with a feature vector $  { \\mathbf x^{(i)}}$ containing counts of words in the vocabulary $ { \\{ x_1^{(i)}, x_2^{(i)},\\ldots, x_m^{(i)}\\}}$\n",
    "\n",
    "* The sum of all feature counts is equal to the total number of words in the document : $ {\\sum \\limits_{j=1}^m x_j^{(i)}=l}$ \n",
    "\n",
    "In mathematical terms: \n",
    "$$ \\mathbf x|y_r \\sim \\text{Multinomial}(w_{1y_r},w_{2y_r},\\ldots,w_{my_r}) $$\n",
    "$$ \\sim \\text {Multinomial}(\\mathbf w_{\\mathbf y_r})$$\n",
    "\n",
    "The **total number of parameters** $=m \\times k + k$ \n",
    "where : \n",
    "* $m \\times k$ is the total number of features for $k$ multinomial distributions and \n",
    "\n",
    "* $k$ is the total number priors.\n",
    "\n",
    "### **Parameter estimation**\n",
    "\n",
    "The $j^{th}$ component of parameters vector $\\mathbf w_{\\mathbf y_r}$ is calculated as follows:\n",
    "\\begin{equation} \n",
    "w_{jy_r}=\\frac{\\sum \\limits_{i=1}^n1(y^{(i)}=y_r)x_j^{(i)}}{\\sum \\limits_{i=1}^n1(y^{(i)}=y_r) \\sum \\limits_{j=1}^m x_j^{(i)}}\n",
    "\\end{equation}\n",
    "\n",
    "Here, \n",
    "\n",
    "* The numerator is the **sum of feature** $x_j$ for all examples from $y_r$.\n",
    "\n",
    "* The denominator is the total count of features from all examples from class $y_r$.\n",
    "\n",
    "With **Laplace correction**: \n",
    "\\begin{equation} \n",
    "w_{jy_r}=\\frac{\\sum \\limits_{i=1}^n1(y^{(i)}=y_r)x_j^{(i)}+\\alpha}{\\sum \\limits_{i=1}^n1(y^{(i)}=y_r) \\sum \\limits_{j=1}^m x_j^{(i)}+m\\alpha}\n",
    "\\end{equation}\n",
    "\n",
    "**NOTE :** We add $\\alpha$ in the numerator and $m\\alpha$ in the denominator, correction of $\\alpha =1$.\n",
    "### **Inference**\n",
    "In log space the calculation is performed as follows \n",
    ":\n",
    "* In the numerator, we first **multiply** the **count matrix** with **transpose of log of weight vector** and **add** it to the **log of prior probabilities** & then **exponentiate** the resulting value.\n",
    "\n",
    "* In the denominator, we perform the same calculation as numerator but for **different class labels**. And **sum** them up.\n",
    "\n",
    "* The denominator normalizes numerator between 0 and 1, thus giving us the posterior probability of label $y_c$ for the given count vector $\\mathbf x$.\n",
    "\n",
    "\\begin{equation} \n",
    "p(y_c|\\mathbf x; l, \\mathbf w_{y_c})=\\frac{\\exp\\left(\\mathbf X(\\log \\mathbf w_{\\mathbf y_r})^T+\\log p(y_c)\\right)}{{\\sum}_r\\exp\\left(\\mathbf X(\\log \\mathbf w_{\\mathbf y_r})^T+\\log p(y_r)\\right)}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNB(object):\n",
    "    def fit(self, X, y, alpha=1):\n",
    "        '''implements parameter estimation for multinomial NB.'''\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        #calculate parameters of k multinomial distributions and priors.\n",
    "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.w_prior = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            #get the total count of features for class c.\n",
    "            total_count = np.sum(np.sum(X_c, axis=1))\n",
    "\n",
    "            #estimate parameters of multinomial distribution for class c\n",
    "\n",
    "            self.w[idx, :] = (np.sum(X_c, axis=0)+alpha) / \\\n",
    "                (total_count+alpha*n_features)\n",
    "\n",
    "            ##estimate class prior for class c.\n",
    "            self.w_prior[idx] = (X_c.shape[0]+alpha)/float(n_samples+alpha*n_classes)\n",
    "\n",
    "    def log_likelihood_prior_prod(self, X):\n",
    "        '''calculates log of product of likelihood and prior.'''\n",
    "        return X@(np.log(self.w).T) + np.log(self.w_prior)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' predicts class for input examples.'''\n",
    "        return np.argmax(self.log_likelihood_prior_prod(X), axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ''' calculates probability of examples belonging to diff. classes.'''\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.exp(q)/np.expand_dims(np.sum(np.exp(q), axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demonstration**\n",
    "\n",
    "We will demonstrate working on least square classification in the following set ups:\n",
    "1. Binary classification set up.\n",
    "\n",
    "2. Multi-class classification set up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEMO 1 : *Binary classification* \n",
    "\n",
    "Generate synthetic data for two classes and each example with 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# range of data 0 to 4\n",
    "X = rng.randint(5, size = (1000,5)) \n",
    "\n",
    "# range of data 0,1\n",
    "y= rng.randint(2,size=(1000,)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix :  (750, 5)\n",
      "Shape of label vector :  (750,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "print('Shape of feature matrix : ', X_train.shape)\n",
    "print('Shape of label vector : ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the parameters of Multinomial NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior :  [0.48138298 0.51861702]\n",
      "\n",
      "Parameters of multinomial distribution : \n",
      " [[0.21458508 0.17993853 0.19027661 0.20648226 0.20871752]\n",
      " [0.21238047 0.19778561 0.19954706 0.19401107 0.19627579]]\n"
     ]
    }
   ],
   "source": [
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "\n",
    "# Examine the parameters of multinomial NB.\n",
    "print('Prior : ',multinomial_nb.w_prior)\n",
    "print()\n",
    "print('Parameters of multinomial distribution : \\n',multinomial_nb.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that : \n",
    "\n",
    "* Each class is equally likely - each class has probability of 0.5.\n",
    "\n",
    "* Sum of probabilities of different features for each class = 1.\n",
    "\n",
    "Let's evaluate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.28      0.36       124\n",
      "           1       0.50      0.71      0.59       126\n",
      "\n",
      "    accuracy                           0.50       250\n",
      "   macro avg       0.49      0.49      0.47       250\n",
      "weighted avg       0.49      0.50      0.47       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, multinomial_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower values of precision and recall is due to the random label assignment in the synthetic data.\n",
    "\n",
    "Let's calculate the probability of each example belonging to both the classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47182646, 0.52817354],\n",
       "       [0.48945212, 0.51054788],\n",
       "       [0.4673714 , 0.5326286 ],\n",
       "       [0.47109459, 0.52890541],\n",
       "       [0.48856185, 0.51143815]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_nb.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEMO 2 : *Multi-class classification*\n",
    "\n",
    "Let's generate data for 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5,size=(1000,5))\n",
    "y = rng.randint(3,size=(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 5), (250, 5), (750,), (250,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X,y)\n",
    "X_train.shape ,X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate parameters of Multinomial NB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior :  [0.34130146 0.35192563 0.30677291]\n",
      "\n",
      "Parameters of multinomial distribution : \n",
      " [[0.21068939 0.197134   0.1859024  0.20565453 0.20061967]\n",
      " [0.21341463 0.18407012 0.20121951 0.19893293 0.2023628 ]\n",
      " [0.21391304 0.2        0.19434783 0.19695652 0.19478261]]\n"
     ]
    }
   ],
   "source": [
    "multinomial_nb = MultinomialNB() \n",
    "multinomial_nb.fit(X_train,y_train)\n",
    "\n",
    "# Examine the parameters of multinomial NB.\n",
    "\n",
    "print('Prior : ',multinomial_nb.w_prior)\n",
    "print()\n",
    "print('Parameters of Multinomial distribution : \\n',multinomial_nb.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the classifier that we have learnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.37      0.36        91\n",
      "           1       0.34      0.62      0.44        78\n",
      "           2       0.08      0.01      0.02        81\n",
      "\n",
      "    accuracy                           0.33       250\n",
      "   macro avg       0.26      0.33      0.27       250\n",
      "weighted avg       0.26      0.33      0.28       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, multinomial_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower values of precision and recall is due to the random label assignment in the synthetic data. \n",
    "\n",
    "Finally predict probability for test examples belonging to different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31618901, 0.35900142, 0.32480957],\n",
       "       [0.35406345, 0.31794449, 0.32799207],\n",
       "       [0.32437015, 0.39520632, 0.28042353],\n",
       "       [0.30468286, 0.35292255, 0.34239459],\n",
       "       [0.31898534, 0.36681071, 0.31420395]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_nb.predict_proba(X_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32f02dc107888b055323539767db1f9cf579f03b0ed3a3ace7986ed2d38433ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
