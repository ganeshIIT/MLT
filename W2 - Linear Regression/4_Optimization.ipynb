{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported for proper rendering of latex in colab.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Math, Latex\n",
    "import numpy as np\n",
    "\n",
    "# Import for generating plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N4 : Optimization\n",
    "The objective of this notebook is to implement optimization component of linear regression model.\n",
    "\n",
    "It is implemented with one of the following two methods:\n",
    "* Normal equation method : Sets the partial derivative of the loss function w.r.t. weight vector to 0 and solves the resulting equation to obtain the weight vector.\n",
    "\n",
    "* Gradient descent method : Iteratively adjusts the weight vector based on the learning rate and the gradient of loss function at the current weight vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Normal equation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    ''' Estimates parameters of the linear regression model with normal eqn.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Actual label vector.\n",
    "        Returns:\n",
    "            weight vector\n",
    "    '''\n",
    "\n",
    "    return np.linalg.pinv(X) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this function with the generated training set whose vector is known to use. \n",
    "* We set up the test with feature matrix, label vector and the expected weight vectors.\n",
    "\n",
    "* Next we estimate the weight vector with ```normal_equation``` function.\n",
    "\n",
    "* We test(a) shape and (b) match between expected and estimated weight vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 5 \n",
    "w0 = 6 \n",
    "n = 100\n",
    "X = np.random.rand(n,)\n",
    "y = w0+w1*X+np.random.randn(n,)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_normal_equation (__main__.TestNormalEquation)\n",
      "Test case for weight estimation for linear regression with normal equation method. ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_normal_equation (__main__.TestNormalEquation)\n",
      "Test case for weight estimation for linear regression with normal equation method.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\faizan\\AppData\\Local\\Temp/ipykernel_31772/1352503705.py\", line 14, in test_normal_equation\n",
      "    estimated_weight_vector = normal_equation(feature_matrix, label_vector)\n",
      "  File \"C:\\Users\\faizan\\AppData\\Local\\Temp/ipykernel_31772/1977596150.py\", line 11, in normal_equation\n",
      "    return np.linalg.pinv(X) @ y\n",
      "  File \"<__array_function__ internals>\", line 5, in pinv\n",
      "  File \"c:\\Users\\faizan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 2002, in pinv\n",
      "    u, s, vt = svd(a, full_matrices=False, hermitian=hermitian)\n",
      "  File \"<__array_function__ internals>\", line 5, in svd\n",
      "  File \"c:\\Users\\faizan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 1641, in svd\n",
      "    _assert_stacked_2d(a)\n",
      "  File \"c:\\Users\\faizan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 196, in _assert_stacked_2d\n",
      "    raise LinAlgError('%d-dimensional array given. Array must be '\n",
      "numpy.linalg.LinAlgError: 1-dimensional array given. Array must be at least two-dimensional\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 3.739s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x164b6e284c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest \n",
    "class TestNormalEquation(unittest.TestCase):\n",
    "\n",
    "    def test_normal_equation(self):\n",
    "\n",
    "        ''' Test case for weight estimation for linear regression with normal equation method.'''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix =  X_train\n",
    "        label_vector = y_train \n",
    "        expected_weight_vector = np.array([5.,6.])\n",
    "\n",
    "        #call\n",
    "        estimated_weight_vector = normal_equation(feature_matrix, label_vector)\n",
    "\n",
    "        # asserts\n",
    "        # test the shape\n",
    "        self.assertEqual(estimated_weight_vector.shape, (2, ))\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(estimated_weight_vector , expected_weight_vector , decimal=0)\n",
    "      \n",
    "unittest.main(argv=[''],defaultTest='TestNormalEquation',verbosity=2,exit=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient descent (GD):\n",
    "\n",
    "GD is implemented as follows:\n",
    "* Randomly initialize $\\textbf w$ to $\\textbf 0$.\n",
    "\n",
    "* Iterate until convergence:\n",
    "    * Calculate partial derivative of loss w.r.t weight vector.\n",
    "\n",
    "    * Calculate new values of weights.\n",
    "\n",
    "    * Update weights to new values $\\textit {simultaneously}$. \n",
    "    \n",
    "We use number of epochs as a convergence criteria in this implementation.\n",
    "\n",
    "#### *Partial derivative of loss function :*\n",
    "\n",
    "Let's first implement a function to calculate partial derivative of loss funciton, which is obtained with the following equation.\n",
    "\\begin{equation} \n",
    "\\frac{∂}{∂ \\textbf w}  J(\\textbf w)= \\textbf X^T(\\textbf{Xw}-\\textbf y)\n",
    "\\end{equation} \n",
    "\n",
    "The multiplication of transpose of feature matrix with the difference of predicted and actual label  vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    assert X.shape[-1] == w.shape[0], \"X and w don't have compatible dimensions\"\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X,y,w):\n",
    "    \n",
    "    ''' Calculates gradients of loss function w.r.t weight vector on training set.\n",
    "        Arguments:\n",
    "            X: Features matrix for training data.\n",
    "            y: Label vector for training data.\n",
    "            w: Weight vector\n",
    "        Retruns:\n",
    "            A vector of gradients.\n",
    "    '''\n",
    "    return np.transpose(X)@(predict(X,w)-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write  a test case for gradient calculation wtih the following setup:\n",
    "1. Feature matrix $(\\mathbf X)$\n",
    "\\begin{equation} \n",
    "\\textbf X_{2 \\times 4} = \\begin{bmatrix} 1&3&2&5\\\\ \n",
    "1&9&4&7\\end{bmatrix} \n",
    "\\end{equation}\n",
    "2. weight vector $(\\textbf w)$\n",
    "\\begin{equation} \\textbf w_{4\\times 1}= \\begin{bmatrix}1\\\\1\\\\1\\\\1 \\end{bmatrix} \n",
    "\\end{equation}\n",
    "3. Label Vector $(y)$: \n",
    "\\begin{equation} y_{2 \\times 1} =  \\begin{bmatrix} 6\\\\11   \\end{bmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Let's compute the partial derivative of loss $ J(\\mathbf w) $ i.e\n",
    "\\begin{align} \\frac{∂}{∂ \n",
    "\\textbf w}  J(\\textbf w)&=& \\textbf X^T(\\textbf{Xw}-\\textbf y) \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\\\ &=&\\begin{bmatrix} 15\\\\105\\\\50\\\\95 \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_calculate_gradient (__main__.TestCalculateGradient)\n",
      "Test case for gradient calculation. ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x164d284ac40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestCalculateGradient(unittest.TestCase):\n",
    "    def test_calculate_gradient(self):\n",
    "        ''' Test case for gradient calculation. '''\n",
    "        #set up\n",
    "        feature_matrix = np.array([[1, 3, 2, 5], [1, 9, 4, 7]])\n",
    "        weight_vector = np.array([1, 1, 1, 1])\n",
    "        label_vector = np.array([6, 11])\n",
    "        expected_grad = np.array([15, 105, 50, 95])\n",
    "\n",
    "        #call\n",
    "        grad = calculate_gradient(feature_matrix, label_vector, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(grad.shape, (4, ))\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_grad, grad, decimal=0)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestCalculateGradient', verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Step : *Weight Updation*\n",
    "We obtain the new weight from the old one by substracting gradient weighted by the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, grad, lr):\n",
    "    ''' Updates the weights based on the gradient of loss function.\n",
    "\n",
    "        Args:\n",
    "            1. w: weight vector\n",
    "            2. grad: gradient of loss wrt w\n",
    "            3. lr: learning rate (alpha)\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "    '''\n",
    "    return (w - lr*grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's workout a weight update for :\n",
    "1. Weight vector $(\\textbf w)$: \n",
    "\\begin{equation} \n",
    "\\textbf w_{4\\times 1}=\\begin{bmatrix} 1\\\\1\\\\1\\\\1 \\end{bmatrix} \n",
    "\\end{equation} \n",
    "2. grad vector $(\\textbf g)$: \n",
    "\\begin{equation} \n",
    "\\textbf g_{4\\times 1}=\\begin{bmatrix} 15\\\\105\\\\50\\\\95 \\end{bmatrix} \n",
    "\\end{equation}\n",
    "3. Learning rate = 10^-3\n",
    "\n",
    "The updated weights are given by:\n",
    "\n",
    "\\begin{align} \n",
    "\\textbf w&=&\\textbf w_{old} -\\alpha \\mathbf g \n",
    "\\end{align} \n",
    "\n",
    "\\begin{align} \n",
    "\\\\&=& \n",
    "\\begin{bmatrix} \n",
    "1\\\\1\\\\1\\\\1\\\\ \\end{bmatrix} -0.001\\times \n",
    "\\begin{bmatrix} \n",
    "15\\\\105\\\\50\\\\95 &\n",
    "\\end{bmatrix}\n",
    "\\end{align} \n",
    "\n",
    "\\begin{align} \n",
    "\\\\ &=& \\begin{bmatrix} 1-0.015\\\\1-0.105\\\\1-0.05\\\\1-0.095 \\end{bmatrix} \n",
    "\\end{align} \n",
    "\n",
    "\\begin{align} \n",
    "\\\\&=& \\begin{bmatrix} 0.985\\\\0.895\\\\0.95\\\\0.095 \n",
    "\\end{bmatrix} \n",
    "\\end{align} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_update_weights (__main__.TestUpdateWeights)\n",
      "Test case for weight update in GD ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x164d2855c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestUpdateWeights(unittest.TestCase):\n",
    "    def test_update_weights(self):\n",
    "        ''' Test case for weight update in GD'''\n",
    "        #set up\n",
    "        weight_vector = np.array([1, 1, 1, 1])\n",
    "        grad_vector = np.array([15, 105, 50, 95])\n",
    "        lr = 0.001\n",
    "        expected_w_new = np.array([0.985, 0.895, 0.95, 0.905])\n",
    "\n",
    "        #call\n",
    "        w_new = update_weights(weight_vector, grad_vector, lr)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(expected_w_new.shape, (4,))\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_w_new, w_new, decimal=1)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestUpdateWeights',\n",
    "              verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Step : *Gradient Descent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w):\n",
    "    e = (predict(X, w)) - y\n",
    "    return (1/2)*((np.transpose(e))@e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X: np.ndarray, y: np.ndarray, lr: float, num_epochs: int):\n",
    "\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    print()\n",
    "    for i in np.arange(0, num_epochs):\n",
    "        w_all.append(w)\n",
    "        err_all.append(loss(X, y, w))\n",
    "        dJdW = calculate_gradient(X, y, w)\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            print('Iteration #:%d,loss: %4.2f' % (i, err_all[-1]))\n",
    "        w = update_weights(w, dJdW, lr)\n",
    "    return w, err_all, w_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test this function, we will use the synthetic training data that was generated earlier. We know the acutal weigths, that can be compared against the weights obtained from gradient descent procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 5 \n",
    "w0 = 6 \n",
    "n = 100\n",
    "X = np.random.rand(n,)\n",
    "y = w0+w1*X+np.random.randn(n,)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_gradient_descent (__main__.TestGradientDescent)\n",
      "Test case for weight update in GD ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_gradient_descent (__main__.TestGradientDescent)\n",
      "Test case for weight update in GD\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\faizan\\AppData\\Local\\Temp/ipykernel_31772/4092288632.py\", line 12, in test_gradient_descent\n",
      "    w, err_all, w_all = gradient_descent(\n",
      "  File \"C:\\Users\\faizan\\AppData\\Local\\Temp/ipykernel_31772/3006440603.py\", line 5, in gradient_descent\n",
      "    w = np.zeros((X.shape[1]))\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x164d27fe3a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestGradientDescent(unittest.TestCase):\n",
    "    def test_gradient_descent(self):\n",
    "        ''' Test case for weight update in GD'''\n",
    "\n",
    "        ''' Test case for gradient calculation. '''\n",
    "        #set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weight = np.array([5., 6.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = gradient_descent(feature_matrix, label_vector, lr=0.001, num_epochs=2000)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(w.shape, (2, ))\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_weight, w, decimal=0)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestGradientDescent',verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Mini Batch Gradient Descent*\n",
    "\n",
    "The key idea here to perform weight updates by computing gradient on batches of small number of examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 200, 100000\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gd(X: np.ndarray, y: np.ndarray, num_iters: int, minibatch_size: int):\n",
    "    ''' Estimates parameters of linear regression model through gradient descent.\n",
    "\n",
    "        Args: \n",
    "        X: feature matrix for training data\n",
    "        y: label vector  for training data\n",
    "        num_iters: number of iterations.\n",
    "\n",
    "        Returns:\n",
    "        weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        weight vectors across different iterations\n",
    "    '''\n",
    "\n",
    "    w_all = []  # all parameters across iterations.\n",
    "    err_all = []  # error across iterations\n",
    "\n",
    "    #parameter vector initialized to [0,0]\n",
    "    w = np.zeros((X.shape[1]))\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(num_iters):\n",
    "\n",
    "        shuffled_indices = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            t += 1\n",
    "            xi = X_shuffled[i, i + minibatch_size]\n",
    "\n",
    "            yi = y_shuffled[i, i + minibatch_size]\n",
    "\n",
    "            err_all.append(loss(xi, yi, w))\n",
    "\n",
    "            gradients = 2/minibatch_size * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(t)\n",
    "\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TestMiniBatchGradientDescent (unittest.loader._FailedTest) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: TestMiniBatchGradientDescent (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute 'TestMiniBatchGradientDescent'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x164d27fef70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_minibatch_gd(self):\n",
    "\n",
    "    #set up \n",
    "    feature_matrix = X_train \n",
    "    label_vector = y_train \n",
    "    expected_weights = np.array([4.,3.])\n",
    "\n",
    "\n",
    "    #call \n",
    "    w, err_all, w_all = mini_batch_gd(feature_matrix , label_vector ,200 , 8)\n",
    "\n",
    "    #asserts \n",
    "    #test the shape \n",
    "    self.assertEqual(w.shape ,(2,))\n",
    "\n",
    "    #and contents \n",
    "    np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestMiniBatchGradientDescent',verbosity=2,exit=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32f02dc107888b055323539767db1f9cf579f03b0ed3a3ace7986ed2d38433ec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
